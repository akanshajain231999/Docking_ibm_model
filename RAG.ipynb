{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48hO1_V_JRG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Building an AI-powered multimodal RAG system with Docling\n",
        "*Using IBM Granite vision, text-based embeddings and generative AI models*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYjGpqcgXR9W"
      },
      "source": [
        "## Lab 3: From Documents to Transparent AI - The Complete Journey\n",
        "\n",
        "Welcome to the final lab in our Docling workshop series! You've come a long way:\n",
        "- **Lab 1**: You learned to convert documents while preserving structure\n",
        "- **Lab 2**: You mastered intelligent chunking strategies\n",
        "- **Lab 3**: Now, we'll build a complete, production-ready RAG system with a game-changing feature: visual grounding\n",
        "\n",
        "This lab represents the culmination of everything you've learned, showing how Docling enables not just document processing, but truly transparent AI systems.\n",
        "\n",
        "## Why This Lab Matters\n",
        "\n",
        "Traditional RAG systems have a trust problem. When an AI provides information, users often wonder:\n",
        "- \"Where did this information come from?\"\n",
        "- \"Can I verify this is accurate?\"\n",
        "- \"Is the AI hallucinating or using real data?\"\n",
        "\n",
        "**Visual grounding solves this problem** by showing users exactly where information was retrieved from in the original documents. This isn't just a nice-to-have feature - it's essential for:\n",
        "-  **Healthcare**: Verify medical information sources\n",
        "-  **Legal**: Trace citations to exact document locations\n",
        "-  **Financial**: Audit AI-generated financial insights\n",
        "-  **Research**: Validate scientific claims\n",
        "-  **Enterprise**: Build trustworthy internal AI systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIV_JkOXw9q"
      },
      "source": [
        "## What Makes This Lab Special\n",
        "\n",
        "We're not just building another RAG system. We're creating a **multimodal RAG system with visual grounding** that:\n",
        "\n",
        "1. **Processes Multiple Data Types**: Text, tables, and images from your documents\n",
        "2. **Shows Exact Sources**: Highlights the precise location of retrieved information\n",
        "3. **Understands Images**: Uses AI vision models to comprehend visual content\n",
        "4. **Maintains Transparency**: Every answer can be visually verified\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAmR3BxvZ2Y0"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJl_fmofYBmH"
      },
      "source": [
        "## Understanding Multimodal RAG with Visual Grounding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghh3uCNMCFwU"
      },
      "source": [
        "### What is Multimodal RAG?\n",
        "\n",
        "[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is a technique used with large language models (LLMs) to connect the model with a knowledge base of information outside the data the LLM has been trained on without having to perform [fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning). Traditional RAG is limited to text-based use cases such as text summarization and chatbots.\n",
        "\n",
        "Traditional RAG systems only handle text. But real documents contain:\n",
        "- **Text**: Paragraphs, lists, headers\n",
        "- **Tables**: Structured data, financial information\n",
        "- **Images**: Charts, diagrams, photos, illustrations\n",
        "\n",
        "Multimodal RAG can use [multimodal LLMs](https://www.ibm.com/think/topics/multimodal-ai) (MLLM) to process information from multiple types of data to be included as part of the external knowledge base used in RAG. Multimodal data can include text, images, audio, video or other forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxgNPDCZEDW"
      },
      "source": [
        "### Visual Grounding: The Transparency Layer\n",
        "\n",
        "Visual grounding adds a crucial transparency layer to RAG systems. When the system retrieves information to answer a query, it doesn't just return text - it shows you exactly where in the original document that information came from by:\n",
        "- Drawing bounding boxes on document pages\n",
        "- Highlighting specific regions\n",
        "- Labeling content types (TEXT, TABLE, IMAGE)\n",
        "- Using different colors for multiple sources\n",
        "\n",
        "For this recipe, you will use IBM Granite models capable of processing different modalities, enhanced with Docling's visual grounding capabilities to create a transparent, verifiable AI system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_S93MsAZz1S"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eDALN1A9LF8"
      },
      "source": [
        "## Recipe Overview\n",
        "\n",
        "In this comprehensive lab, you'll learn how to:\n",
        "\n",
        "1. **Configure Docling for Visual Grounding**: Set up document processing to maintain visual references\n",
        "2. **Process Multimodal Content**: Handle text, tables, and images with proper metadata\n",
        "3. **Leverage AI Vision Models**: Use IBM Granite vision models to understand images\n",
        "4. **Build a Vector Database**: Store embeddings with visual grounding metadata\n",
        "5. **Implement Visual Attribution**: Show users exactly where information comes from\n",
        "6. **Create a Complete RAG Pipeline**: Combine all components into a production-ready system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSIhob4jZcAN"
      },
      "source": [
        "### Technologies We'll Use\n",
        "\n",
        "Building on our previous labs, we'll add:\n",
        "\n",
        "1. **[Docling](https://docling-project.github.io/docling/):** An open-source toolkit used to parse and convert documents.\n",
        "2. **[LangChain](https://langchain.com)**: For orchestrating the RAG pipeline\n",
        "3. **[IBM Granite Vision Models](https://www.ibm.com/granite/)**: For understanding image content\n",
        "4. **Visual Grounding**: Docling's unique capability for source attribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlsFgvGZ9hH"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooxv7ltEZBf"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before we begin, ensure you have:\n",
        "- Completed Labs 1 and 2 (or equivalent Docling knowledge)\n",
        "- Python 3.11 or 3.12 installed\n",
        "- Basic understanding of embeddings and vector databases\n",
        "- Familiarity with the concepts from previous labs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN2mK175_JRH",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfg8kTVr_JRH"
      },
      "source": [
        "Ensure you are running Python 3.11 or 3.12 in a freshly created virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEoM938B_JRH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 11) and sys.version_info < (3, 13), \"Use Python 3.11 or 3.12 to run this notebook.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p_2cX1-_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Now let's install all necessary packages. Note that we're building on the packages from previous labs and adding vision and RAG-specific components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BfMWUUSs_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! echo \"::group::Install Dependencies\"\n",
        "%pip install uv\n",
        "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
        "    transformers \\\n",
        "    pillow \\\n",
        "    langchain_community \\\n",
        "    'langchain_huggingface[full]' \\\n",
        "    langchain_milvus 'pymilvus[milvus_lite]'\\\n",
        "    docling \\\n",
        "    replicate \\\n",
        "    matplotlib\n",
        "! echo \"::endgroup::\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gu-Oeay_JRJ"
      },
      "source": [
        "## Import Required Libraries and Configure Logging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ade2DMFeCFwW"
      },
      "source": [
        "### Logging Configuration\n",
        "\n",
        "To see detailed information about the document processing and visual grounding operations, we'll configure INFO log level.\n",
        "\n",
        "NOTE: It is okay to skip running this cell if you prefer less verbose output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ke7lF4CFwW"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA4Vz3k5HtgI"
      },
      "source": [
        "### Required Libraries\n",
        "\n",
        "Now let's import all necessary modules, organized by their purpose:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJwoqaBPHySg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import base64\n",
        "import io\n",
        "import itertools\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "from PIL import ImageDraw\n",
        "from IPython.display import display\n",
        "\n",
        "# Docling imports for document processing and visual grounding\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.datamodel.document import DoclingDocument\n",
        "from docling.chunking import DocMeta\n",
        "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
        "from docling_core.types.doc.document import TableItem, RefItem\n",
        "from docling_core.types.doc.labels import DocItemLabel\n",
        "\n",
        "# LangChain imports for RAG pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import Replicate\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_milvus import Milvus\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Model imports\n",
        "from transformers import AutoTokenizer, AutoProcessor\n",
        "from ibm_granite_community.notebook_utils import get_env_var, escape_f_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yodQeEWVa3Xa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyixsnLzCFwW"
      },
      "source": [
        "## Selecting and loading the AI models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB_Hry0ebAtE"
      },
      "source": [
        "### The Three Pillars of Multimodal RAG\n",
        "\n",
        "For a complete multimodal RAG system with visual grounding, we need three types of models, each serving a crucial purpose:\n",
        "\n",
        "1. **Embeddings Model**: Converts text into vector representations\n",
        "   - Enables semantic search (\"find content similar in meaning\")\n",
        "   - Must handle text from chunks, tables, and image descriptions\n",
        "\n",
        "2. **Vision Model**: Understands and describes visual content\n",
        "   - Processes images found in documents\n",
        "   - Generates textual descriptions for retrieval\n",
        "\n",
        "3. **Language Model**: Generates final responses\n",
        "   - Synthesizes retrieved information\n",
        "   - Produces coherent, accurate answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEIpVe6yIAN6"
      },
      "source": [
        "### Load the Granite Embeddings Model\n",
        "\n",
        "We'll use the [Granite Embeddings model](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb) for generating text embedding vectors.\n",
        "\n",
        "**Why this model**:\n",
        "- Optimized for English text\n",
        "- Compact (30M parameters) for fast processing\n",
        "- Excellent semantic understanding\n",
        "- 512 token context window\n",
        "\n",
        "To use a different embeddings model, you can refer to [this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvztNZly_JRJ"
      },
      "outputs": [],
      "source": [
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embeddings_model_path,\n",
        ")\n",
        "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)\n",
        "print(f\"Embeddings model loaded: {embeddings_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9nTm3qICFwW"
      },
      "source": [
        "### Load the Granite Vision Model\n",
        "\n",
        "The vision model will help us understand images within documents. This is crucial for truly multimodal RAG as many documents contain important visual information.\n",
        "\n",
        "**Why use a hosted model**:\n",
        "- Faster processing without local GPU requirements\n",
        "- Consistent performance\n",
        "- Easy to scale\n",
        "\n",
        "**Note**: To set up Replicate, see [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb).\n",
        "\n",
        "\n",
        "To connect to a model on a provider other than Replicate, see the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5272bCOCFwW"
      },
      "outputs": [],
      "source": [
        "vision_model_path = \"ibm-granite/granite-vision-3.3-2b\"\n",
        "vision_model = Replicate(\n",
        "    model=vision_model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": embeddings_tokenizer.max_len_single_sentence,\n",
        "        \"min_tokens\": 100,\n",
        "        \"temperature\": 0.01 # low temperature for reproduceability\n",
        "    },\n",
        ")\n",
        "vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n",
        "print(f\"Vision model loaded: {vision_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8eWR10_JRJ"
      },
      "source": [
        "### Load the Granite Language Model\n",
        "\n",
        "Finally, our language model will generate responses based on retrieved context.\n",
        "\n",
        "**Why this model**:\n",
        "- 8B parameters: Good balance of quality and speed\n",
        "- Instruction-tuned: Follows prompts accurately\n",
        "- Granite family: Open-source and Apache 2.0 licensed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckyj7Zrh_JRK"
      },
      "outputs": [],
      "source": [
        "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
        "model = Replicate(\n",
        "    model=model_path,\n",
        "    replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n",
        "    model_kwargs={\n",
        "        \"max_tokens\": 1000,\n",
        "        \"min_tokens\": 100,\n",
        "        \"temperature\": 0.01 # low temperature for reproduceability\n",
        "\n",
        "    },\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "print(f\"Language model loaded: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVfNx7tCcB8l"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nviHG3n7_JRK",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Enhanced document processing with visual grounding support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0TSvvRFJTDJ"
      },
      "source": [
        "### The Foundation of Visual Grounding\n",
        "\n",
        "Visual grounding requires special configuration during document conversion. Unlike standard processing, we need to:\n",
        "\n",
        "1. **Generate high-quality page images**: For displaying visual highlights\n",
        "2. **Preserve coordinate information**: To know where content is located\n",
        "3. **Maintain document structure**: For accurate source attribution\n",
        "4. **Store documents properly**: For later retrieval and visualization\n",
        "\n",
        "Let's configure Docling with these requirements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3jGNMkKJZ-R"
      },
      "outputs": [],
      "source": [
        "# Configure converter with visual grounding capabilities\n",
        "pdf_pipeline_options = PdfPipelineOptions(\n",
        "    do_ocr=False,  # Set to True if your PDFs contain scanned images\n",
        "    generate_picture_images=True,  # Extract images from documents\n",
        "    generate_page_images=True,  # CRITICAL: Generate page images for visual grounding\n",
        ")\n",
        "\n",
        "format_options = {\n",
        "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
        "}\n",
        "\n",
        "converter = DocumentConverter(format_options=format_options)\n",
        "print(\"Document converter configured with visual grounding support\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ7Guu7A_JRK"
      },
      "source": [
        "### Create a Document Store for Visual Grounding\n",
        "\n",
        "The document store will maintain the full document structure needed for visual grounding. This is essential for highlighting source locations later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNGz_0gZ_JRK"
      },
      "outputs": [],
      "source": [
        "# Create document store for visual grounding\n",
        "doc_store = {}\n",
        "doc_store_root = Path(mkdtemp())\n",
        "print(f\"Document store created at: {doc_store_root}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSLYgc7JCFwX"
      },
      "source": [
        "### Convert Documents with Visual Tracking\n",
        "\n",
        "Now let's process documents while preserving all information needed for visual grounding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLDMCxFbCFwX"
      },
      "outputs": [],
      "source": [
        "sources = [\n",
        "    \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\",\n",
        "    # Add more document URLs here as needed\n",
        "]\n",
        "\n",
        "conversions = {}\n",
        "\n",
        "print(\"Starting document conversion with visual grounding support...\")\n",
        "for source in sources:\n",
        "    print(f\"\\n Processing: {source}\")\n",
        "\n",
        "    # Convert document\n",
        "    result = converter.convert(source=source)\n",
        "    docling_document = result.document\n",
        "    conversions[source] = docling_document\n",
        "\n",
        "    # Save document to store for visual grounding\n",
        "    # The binary hash ensures unique identification\n",
        "    file_path = doc_store_root / f\"{docling_document.origin.binary_hash}.json\"\n",
        "    docling_document.save_as_json(file_path)\n",
        "    doc_store[docling_document.origin.binary_hash] = file_path\n",
        "\n",
        "    print(f\"Document converted and saved\")\n",
        "    print(f\"  - Document ID: {docling_document.origin.binary_hash}\")\n",
        "    print(f\"  - Pages: {len(docling_document.pages)}\")\n",
        "    print(f\"  - Tables: {len(docling_document.tables)}\")\n",
        "    print(f\"  - Images: {len(docling_document.pictures)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vX3XqSOKBos"
      },
      "source": [
        "## Process Document Content with Visual Grounding Metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dvccMBeKC7c"
      },
      "source": [
        "### The Importance of Metadata Preservation\n",
        "\n",
        "For visual grounding to work, every chunk of content must maintain metadata about its source location. This includes:\n",
        "- **Page numbers**: Which page(s) contain this content\n",
        "- **Bounding boxes**: Exact coordinates on the page\n",
        "- **Document references**: Links back to the source document\n",
        "- **Content type**: Whether it's text, table, or image\n",
        "\n",
        "This metadata is what enables us to draw accurate highlights on document pages later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdT7gzIeKFyd"
      },
      "source": [
        "### Process Text Chunks with Location Tracking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zgnHW4UCFwX"
      },
      "source": [
        "Next we process any tables in the documents. We convert the table data to markdown format for passing into the language model. A list of LangChain documents are created from the table's markdown renderings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDa6yik4CFwX"
      },
      "outputs": [],
      "source": [
        "from docling.chunking import DocMeta\n",
        "\n",
        "doc_id = 0\n",
        "texts: list[Document] = []\n",
        "\n",
        "print(\"\\nProcessing text chunks with visual grounding metadata...\")\n",
        "for source, docling_document in conversions.items():\n",
        "    chunker = HybridChunker(tokenizer=embeddings_tokenizer)\n",
        "\n",
        "    for chunk in chunker.chunk(docling_document):\n",
        "        items = chunk.meta.doc_items\n",
        "\n",
        "        # Skip single-item chunks that are tables (we'll process them separately)\n",
        "        if len(items) == 1 and isinstance(items[0], TableItem):\n",
        "            continue\n",
        "\n",
        "        refs = \" \".join(map(lambda item: item.get_ref().cref, items))\n",
        "        text = chunk.text\n",
        "\n",
        "        # Create document with enhanced metadata for visual grounding\n",
        "        document = Document(\n",
        "            page_content=text,\n",
        "            metadata={\n",
        "                \"doc_id\": (doc_id:=doc_id+1),\n",
        "                \"source\": source,\n",
        "                \"ref\": refs,  # References for tracking specific document items\n",
        "                \"dl_meta\": chunk.meta.model_dump(),  # CRITICAL: Store chunk metadata for visual grounding\n",
        "                \"origin_hash\": docling_document.origin.binary_hash  # Link to stored document\n",
        "            },\n",
        "        )\n",
        "        texts.append(document)\n",
        "\n",
        "print(f\"Created {len(texts)} text chunks with visual grounding metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzoFPRBjCFwX"
      },
      "source": [
        "### Process Tables with Spatial Information\n",
        "\n",
        "Tables require special handling to preserve their structure and location:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaiUN8nVCFwX"
      },
      "outputs": [],
      "source": [
        "doc_id = len(texts)\n",
        "tables: list[Document] = []\n",
        "\n",
        "print(\"\\nProcessing tables...\")\n",
        "for source, docling_document in conversions.items():\n",
        "    for table in docling_document.tables:\n",
        "        if table.label in [DocItemLabel.TABLE]:\n",
        "            ref = table.get_ref().cref\n",
        "            text = table.export_to_markdown(docling_document)\n",
        "\n",
        "            # Extract provenance information for visual grounding\n",
        "            prov_data = []\n",
        "            if hasattr(table, 'prov') and table.prov:\n",
        "                for prov in table.prov:\n",
        "                    # Get the page to access its height for coordinate conversion\n",
        "                    if prov.page_no < len(docling_document.pages):\n",
        "                        page = docling_document.pages[prov.page_no]\n",
        "                        # Convert to top-left origin and normalize\n",
        "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
        "                        bbox_norm = bbox.normalized(page.size)\n",
        "\n",
        "                        prov_data.append({\n",
        "                           \"page_no\": prov.page_no,\n",
        "                           \"bbox\": {\n",
        "                              \"l\": bbox_norm.l,  # Use normalized coordinates\n",
        "                              \"t\": bbox_norm.t,\n",
        "                              \"r\": bbox_norm.r,\n",
        "                              \"b\": bbox_norm.b\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id:=doc_id+1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\": ref,\n",
        "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
        "                    \"item_type\": \"table\",  # Mark as table\n",
        "                    \"prov_data\": prov_data  # Store provenance as simple data\n",
        "                },\n",
        "            )\n",
        "            tables.append(document)\n",
        "\n",
        "print(f\"Created {len(tables)} table documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWHbpeEuCFwX"
      },
      "source": [
        "### Process Images with Vision Understanding\n",
        "\n",
        "For true multimodal RAG, we need to understand the content of images. We'll use the Granite vision model to generate descriptions.\n",
        "\n",
        "\n",
        "NOTE: Processing images can take time depending on the number of images and the vision model service. Each image will be analyzed individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htIYVVjHPKSX"
      },
      "outputs": [],
      "source": [
        "def encode_image(image: PIL.Image.Image, format: str = \"png\") -> str:\n",
        "    \"\"\"Encode image to base64 for vision model processing\"\"\"\n",
        "    image = PIL.ImageOps.exif_transpose(image) or image\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format)\n",
        "    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    uri = f\"data:image/{format};base64,{encoding}\"\n",
        "    return uri\n",
        "\n",
        "# Configure vision prompt - feel free to experiment with this!\n",
        "image_prompt = \"Give a detailed description of what is depicted in the image\"\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": image_prompt},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "vision_prompt = vision_processor.apply_chat_template(\n",
        "    conversation=conversation,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "pictures: list[Document] = []\n",
        "doc_id = len(texts) + len(tables)\n",
        "\n",
        "for source, docling_document in conversions.items():\n",
        "    for picture in docling_document.pictures:\n",
        "        ref = picture.get_ref().cref\n",
        "        print(f\"  Processing image: {ref}\")\n",
        "\n",
        "        image = picture.get_image(docling_document)\n",
        "        if image:\n",
        "            # Generate image description using vision model\n",
        "            text = vision_model.invoke(vision_prompt, image=encode_image(image))\n",
        "\n",
        "            # Extract provenance information for visual grounding\n",
        "            prov_data = []\n",
        "            if hasattr(picture, 'prov') and picture.prov:\n",
        "                for prov in picture.prov:\n",
        "                    # Get the page to access its height for coordinate conversion\n",
        "                    if prov.page_no < len(docling_document.pages):\n",
        "                        page = docling_document.pages[prov.page_no]\n",
        "                        # Convert to top-left origin and normalize\n",
        "                        bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
        "                        bbox_norm = bbox.normalized(page.size)\n",
        "\n",
        "                        prov_data.append({\n",
        "                            \"page_no\": prov.page_no,\n",
        "                            \"bbox\": {\n",
        "                                \"l\": bbox_norm.l,\n",
        "                                \"t\": bbox_norm.t,\n",
        "                                \"r\": bbox_norm.r,\n",
        "                                \"b\": bbox_norm.b\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "            document = Document(\n",
        "                page_content=text,\n",
        "                metadata={\n",
        "                    \"doc_id\": (doc_id:=doc_id+1),\n",
        "                    \"source\": source,\n",
        "                    \"ref\": ref,\n",
        "                    \"origin_hash\": docling_document.origin.binary_hash,\n",
        "                    \"item_type\": \"picture\",  # Mark as picture for special handling\n",
        "                    \"prov_data\": prov_data  # Store normalized provenance data\n",
        "                },\n",
        "            )\n",
        "            pictures.append(document)\n",
        "\n",
        "print(f\"Created {len(pictures)} image descriptions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3N4DIVL1a_"
      },
      "source": [
        "### Display Sample Processed Documents\n",
        "\n",
        "Let's examine what we've created to understand the multimodal nature of our system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C_F_3_ZUL1yS"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSample processed documents:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show sample text chunks\n",
        "print(\"\\nTEXT CHUNK EXAMPLES:\")\n",
        "print(\"-\" * 80)\n",
        "for i, text_doc in enumerate(texts[:3]):  # Show first 3 text chunks\n",
        "    print(f\"\\nText Chunk {i+1}:\")\n",
        "    print(f\"  Document ID: {text_doc.metadata['doc_id']}\")\n",
        "    print(f\"  Source: {text_doc.metadata['source'].split('/')[-1]}\")  # Just filename\n",
        "    print(f\"  Reference: {text_doc.metadata['ref']}\")\n",
        "    print(f\"  Has visual grounding: {'dl_meta' in text_doc.metadata}\")\n",
        "    print(f\"  Content preview:\")\n",
        "    print(f\"    {text_doc.page_content[:250]}...\")\n",
        "    if i < 2:  # Add separator between examples except after the last one\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Show sample tables\n",
        "print(\"\\n\\nTABLE EXAMPLES:\")\n",
        "print(\"-\" * 80)\n",
        "if tables:\n",
        "    for i, table_doc in enumerate(tables[:3]):  # Show first 3 tables\n",
        "        print(f\"\\nTable {i+1}:\")\n",
        "        print(f\"  Document ID: {table_doc.metadata['doc_id']}\")\n",
        "        print(f\"  Reference: {table_doc.metadata['ref']}\")\n",
        "        print(f\"  Content preview (Markdown format):\")\n",
        "        # Show first few lines of the table\n",
        "        table_lines = table_doc.page_content.split('\\n')[:8]\n",
        "        for line in table_lines:\n",
        "            print(f\"    {line}\")\n",
        "else:\n",
        "    print(\"  No tables found in the document.\")\n",
        "\n",
        "# Show sample images with descriptions\n",
        "print(\"\\n\\nIMAGE EXAMPLES WITH AI-GENERATED DESCRIPTIONS:\")\n",
        "print(\"-\" * 80)\n",
        "if pictures:\n",
        "    for i, pic_doc in enumerate(pictures[:3]):  # Show first 3 images\n",
        "        print(f\"\\nImage {i+1}:\")\n",
        "        print(f\"  Document ID: {pic_doc.metadata['doc_id']}\")\n",
        "        print(f\"  Reference: {pic_doc.metadata['ref']}\")\n",
        "        print(f\"  AI-Generated Description:\")\n",
        "        # Wrap the description for better readability\n",
        "        import textwrap\n",
        "        wrapped_text = textwrap.fill(pic_doc.page_content, width=70, initial_indent=\"    \", subsequent_indent=\"    \")\n",
        "        print(wrapped_text)\n",
        "\n",
        "        # Display the actual image\n",
        "        source = pic_doc.metadata['source']\n",
        "        ref = pic_doc.metadata['ref']\n",
        "        docling_document = conversions[source]\n",
        "        picture = RefItem(cref=ref).resolve(docling_document)\n",
        "        image = picture.get_image(docling_document)\n",
        "        if image:\n",
        "            print(f\"\\n  Original Image:\")\n",
        "            # Resize image for display if too large\n",
        "            display_image = image.copy()\n",
        "            max_width = 600\n",
        "            if display_image.width > max_width:\n",
        "                ratio = max_width / display_image.width\n",
        "                new_height = int(display_image.height * ratio)\n",
        "                display_image = display_image.resize((max_width, new_height), PIL.Image.Resampling.LANCZOS)\n",
        "            display(display_image)\n",
        "\n",
        "        if i < min(2, len(pictures)-1):\n",
        "            print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"  No images found in the document.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj4QM8GJOiFd"
      },
      "source": [
        "## Visual Grounding Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W292HEjOOkC3"
      },
      "source": [
        "### Understanding Visual Source Attribution\n",
        "\n",
        "Visual grounding is what sets this system apart. These functions enable us to:\n",
        "1. **Locate**: Find the exact source of any retrieved information\n",
        "2. **Highlight**: Draw visual indicators on document pages\n",
        "3. **Differentiate**: Use different styles for text, tables, and images\n",
        "4. **Verify**: Allow users to confirm AI responses against source documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GSYbD1GOkrB"
      },
      "source": [
        "### Implementing the Core Visual Grounding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0SFvP-tOtkE"
      },
      "outputs": [],
      "source": [
        "def visualize_chunk_grounding(chunk, doc_store, highlight_color=\"blue\"):\n",
        "    \"\"\"\n",
        "    Visualize where a text chunk comes from in the original document.\n",
        "\n",
        "    This function:\n",
        "    1. Loads the original document from the store\n",
        "    2. Finds the pages containing the chunk content\n",
        "    3. Draws bounding boxes around the source regions\n",
        "    4. Displays the highlighted pages\n",
        "\n",
        "    Args:\n",
        "        chunk: LangChain Document with visual grounding metadata\n",
        "        doc_store: Dictionary mapping document hashes to file paths\n",
        "        highlight_color: Color for highlighting (blue, green, red, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of page images with highlights\n",
        "    \"\"\"\n",
        "    # Get the origin hash\n",
        "    origin_hash = chunk.metadata.get(\"origin_hash\")\n",
        "    if not origin_hash:\n",
        "        print(\"No origin hash found in metadata\")\n",
        "        return None\n",
        "\n",
        "    # Load the full document from store\n",
        "    dl_doc = DoclingDocument.load_from_json(doc_store.get(origin_hash))\n",
        "\n",
        "    print(f\"Visualizing source location for chunk {chunk.metadata.get('doc_id', 'Unknown')}\")\n",
        "\n",
        "    # Handle different types of content\n",
        "    page_images = {}\n",
        "    item_type = chunk.metadata.get(\"item_type\", \"text\")\n",
        "\n",
        "    if item_type in [\"picture\", \"table\"] and \"prov_data\" in chunk.metadata:\n",
        "        # Handle tables and pictures with simple provenance data\n",
        "        prov_data = chunk.metadata[\"prov_data\"]\n",
        "\n",
        "        if not prov_data:\n",
        "            print(f\"No provenance data available for this {item_type}\")\n",
        "            return None\n",
        "\n",
        "        for prov in prov_data:\n",
        "            page_no = prov[\"page_no\"]\n",
        "\n",
        "            # Get page image\n",
        "            if page_no < len(dl_doc.pages):\n",
        "                page = dl_doc.pages[page_no]\n",
        "                if hasattr(page, 'image') and page.image:\n",
        "                    if page_no not in page_images:\n",
        "                        img = page.image.pil_image.copy()\n",
        "                        page_images[page_no] = {\n",
        "                            'image': img,\n",
        "                            'page': page,\n",
        "                            'draw': ImageDraw.Draw(img)\n",
        "                        }\n",
        "\n",
        "                    # Draw bounding box\n",
        "                    draw = page_images[page_no]['draw']\n",
        "                    bbox = prov[\"bbox\"]\n",
        "\n",
        "                    # Draw bounding box\n",
        "                    draw = page_images[page_no]['draw']\n",
        "                    bbox = prov[\"bbox\"]\n",
        "\n",
        "                    # The coordinates are already normalized and in top-left origin\n",
        "                    # Just scale to image dimensions\n",
        "                    img_width = page_images[page_no]['image'].width\n",
        "                    img_height = page_images[page_no]['image'].height\n",
        "\n",
        "                    l = int(bbox[\"l\"] * img_width)\n",
        "                    r = int(bbox[\"r\"] * img_width)\n",
        "                    t = int(bbox[\"t\"] * img_height)\n",
        "                    b = int(bbox[\"b\"] * img_height)\n",
        "\n",
        "                    # Ensure coordinates are valid (min/max) just in case\n",
        "                    l, r = min(l, r), max(l, r)\n",
        "                    t, b = min(t, b), max(t, b)\n",
        "\n",
        "                    # Clamp to image bounds\n",
        "                    l = max(0, min(l, img_width - 1))\n",
        "                    r = max(0, min(r, img_width - 1))\n",
        "                    t = max(0, min(t, img_height - 1))\n",
        "                    b = max(0, min(b, img_height - 1))\n",
        "\n",
        "                    # Draw highlight with different styles for different types\n",
        "                    if item_type == \"picture\":\n",
        "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=4)\n",
        "                        draw.text((l, t-20), \"IMAGE\", fill=highlight_color)\n",
        "                    elif item_type == \"table\":\n",
        "                        draw.rectangle([l, t, r, b], outline=highlight_color, width=3)\n",
        "                        draw.text((l, t-20), \"TABLE\", fill=highlight_color)\n",
        "\n",
        "    elif \"dl_meta\" in chunk.metadata:\n",
        "        # Handle text chunks with DocMeta\n",
        "        try:\n",
        "            meta = DocMeta.model_validate(chunk.metadata[\"dl_meta\"])\n",
        "\n",
        "            # Process each item in the chunk to find source locations\n",
        "            for doc_item in meta.doc_items:\n",
        "                if hasattr(doc_item, 'prov') and doc_item.prov:\n",
        "                    for prov in doc_item.prov:\n",
        "                        page_no = prov.page_no\n",
        "\n",
        "                        # Get or create page image\n",
        "                        if page_no not in page_images:\n",
        "                            if page_no < len(dl_doc.pages):\n",
        "                                page = dl_doc.pages[page_no]\n",
        "                                if hasattr(page, 'image') and page.image:\n",
        "                                    img = page.image.pil_image.copy()\n",
        "                                    page_images[page_no] = {\n",
        "                                        'image': img,\n",
        "                                        'page': page,\n",
        "                                        'draw': ImageDraw.Draw(img)\n",
        "                                    }\n",
        "\n",
        "                        # Draw bounding box on the page\n",
        "                        if page_no in page_images:\n",
        "                            page_data = page_images[page_no]\n",
        "                            page = page_data['page']\n",
        "                            draw = page_data['draw']\n",
        "\n",
        "                            # Convert coordinates to image space\n",
        "                            bbox = prov.bbox.to_top_left_origin(page_height=page.size.height)\n",
        "                            bbox = bbox.normalized(page.size)\n",
        "\n",
        "                            # Scale to actual image dimensions\n",
        "                            l = int(bbox.l * page_data['image'].width)\n",
        "                            r = int(bbox.r * page_data['image'].width)\n",
        "                            t = int(bbox.t * page_data['image'].height)\n",
        "                            b = int(bbox.b * page_data['image'].height)\n",
        "\n",
        "                            # Draw highlight rectangle\n",
        "                            draw.rectangle([l, t, r, b], outline=highlight_color, width=2)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text chunk metadata: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"No visual grounding metadata available for this chunk\")\n",
        "        return None\n",
        "\n",
        "    # Display highlighted pages\n",
        "    for page_no, page_data in sorted(page_images.items()):\n",
        "        plt.figure(figsize=(12, 16))\n",
        "        plt.imshow(page_data['image'])\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Add title indicating content type\n",
        "        if item_type == \"picture\":\n",
        "            title = \"Image Location\"\n",
        "        elif item_type == \"table\":\n",
        "            title = \"Table Location\"\n",
        "        else:\n",
        "            title = \"Text Location\"\n",
        "        plt.title(f'{title} - Page {page_no + 1}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return page_images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVVgC_YQaln"
      },
      "source": [
        "## Populate the Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wqnAw0Te1zi"
      },
      "source": [
        "### Understanding Vector Databases in Multimodal RAG\n",
        "\n",
        "Vector databases are the search engine of our RAG system. They:\n",
        "- Store numerical representations (embeddings) of our content\n",
        "- Enable semantic similarity search\n",
        "- Maintain all metadata needed for visual grounding\n",
        "- Support fast retrieval at scale\n",
        "\n",
        "For multimodal content, this means:\n",
        "- Text chunks are embedded directly\n",
        "- Table markdown is embedded for structure search\n",
        "- AI-generated image descriptions are embedded for visual search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_0oC5arQbHi"
      },
      "source": [
        "### Choose and Configure Vector Database\n",
        "\n",
        "We'll use Milvus, a high-performance vector database. For other vector database options, see [this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQIOysr3Qgg5"
      },
      "outputs": [],
      "source": [
        "# Create a temporary database file\n",
        "db_file = tempfile.NamedTemporaryFile(prefix=\"vectorstore_\", suffix=\".db\", delete=False).name\n",
        "print(f\"Vector database will be saved to: {db_file}\")\n",
        "\n",
        "# Initialize Milvus vector store\n",
        "vector_db: VectorStore = Milvus(\n",
        "    embedding_function=embeddings_model,\n",
        "    connection_args={\"uri\": db_file},\n",
        "    auto_id=True,\n",
        "    enable_dynamic_field=True,  # Allows flexible metadata storage\n",
        "    index_params={\"index_type\": \"AUTOINDEX\"},  # Automatic index optimization\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy-12B1eQq_P"
      },
      "source": [
        "### Add Documents to Vector Database\n",
        "\n",
        "Now we'll add all our processed documents (text chunks, tables, and image descriptions) to the vector database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlQMqXOEQt2o"
      },
      "outputs": [],
      "source": [
        "print(\"\\nAdding documents to vector database...\")\n",
        "documents = list(itertools.chain(texts, tables, pictures))\n",
        "ids = vector_db.add_documents(documents)\n",
        "print(f\"Successfully added {len(ids)} documents to the vector database\")\n",
        "print(f\"  - Text chunks: {len(texts)}\")\n",
        "print(f\"  - Tables: {len(tables)}\")\n",
        "print(f\"  - Image descriptions: {len(pictures)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xavz2IieQyqI"
      },
      "source": [
        "## Test Retrieval with Visual Grounding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjuJtoCKfPAo"
      },
      "source": [
        "### Understanding Retrieval Testing\n",
        "\n",
        "Before building the full RAG pipeline, let's test that our retrieval and visual grounding work correctly. This helps verify:\n",
        "- Content is being found based on semantic similarity\n",
        "- Visual grounding metadata is preserved\n",
        "- Different content types are handled properly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gLXwOObfSCR"
      },
      "source": [
        "### Basic Retrieval Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuc09LOtQ5IP"
      },
      "outputs": [],
      "source": [
        "# Test query\n",
        "test_query = \"How much was spent on food distribution relative to the amount of food distributed?\"\n",
        "\n",
        "print(f\"\\nTesting retrieval for query: '{test_query}'\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Retrieve relevant documents\n",
        "retrieved_docs = vector_db.as_retriever().invoke(test_query)\n",
        "\n",
        "# Display retrieved documents\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\nRetrieved Document {i+1}:\")\n",
        "\n",
        "    # Determine content type\n",
        "    item_type = doc.metadata.get('item_type', 'text')\n",
        "    if item_type == 'picture':\n",
        "        content_type = \"AI-Generated Image Description\"\n",
        "    elif item_type == 'table':\n",
        "        content_type = \"Table (Markdown)\"\n",
        "    else:\n",
        "        content_type = \"Text Chunk\"\n",
        "\n",
        "    print(f\"Type: {content_type}\")\n",
        "    print(f\"Content preview: {doc.page_content[:200]}...\")\n",
        "    print(f\"Source: {doc.metadata['source'].split('/')[-1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iBcEjhVRBcG"
      },
      "source": [
        "## Enhanced RAG with Visual Grounding - Bringing It All Together\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma2N5kKTRHWO"
      },
      "source": [
        "### The Complete Multimodal RAG Pipeline\n",
        "\n",
        "Now we'll implement the full RAG system that:\n",
        "1. Retrieves relevant multimodal content\n",
        "2. Shows exactly where each piece comes from\n",
        "3. Generates accurate, grounded responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_60Mv9wgfmkm"
      },
      "source": [
        "### Understanding Visual Grounding for Different Content Types\n",
        "\n",
        "Our system handles three content types distinctly:\n",
        "\n",
        "1. **Text Chunks**: Standard highlighting shows text passages\n",
        "2. **Tables**: Thick borders with \"TABLE\" labels mark structured data\n",
        "3. **Images**: Distinctive borders with \"IMAGE\" labels show picture locations\n",
        "\n",
        "This visual differentiation helps users quickly understand what type of content contributed to the AI's response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PRpHSBcJehQ"
      },
      "source": [
        "### Implementing the Complete RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmQdDvcXRD39"
      },
      "outputs": [],
      "source": [
        "def rag_with_visual_grounding(question, vector_db, doc_store, model, tokenizer, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform RAG with visual grounding of results.\n",
        "\n",
        "    This function:\n",
        "    1. Retrieves relevant chunks from the vector database\n",
        "    2. Visualizes where each chunk comes from in the original document\n",
        "    3. Generates a response using the retrieved context\n",
        "\n",
        "    Args:\n",
        "        question: User's query\n",
        "        vector_db: Vector database with embedded documents\n",
        "        doc_store: Document store for visual grounding\n",
        "        model: Language model for response generation\n",
        "        tokenizer: Tokenizer for the language model\n",
        "        top_k: Number of chunks to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (outputs, relevant_chunks)\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Retrieve relevant chunks\n",
        "    print(f\"\\nRetrieving top {top_k} relevant chunks...\")\n",
        "    retriever = vector_db.as_retriever(search_kwargs={\"k\": top_k})\n",
        "    relevant_chunks = retriever.invoke(question)\n",
        "\n",
        "    print(f\"Found {len(relevant_chunks)} relevant chunks\")\n",
        "\n",
        "    # Step 2: Visualize each chunk's source location\n",
        "    print(\"\\nVisualizing source locations:\")\n",
        "\n",
        "    for i, chunk in enumerate(relevant_chunks):\n",
        "        print(f\"\\n--- Result {i+1} ---\")\n",
        "\n",
        "        # Determine content type\n",
        "        item_type = chunk.metadata.get('item_type', 'text')\n",
        "        if item_type == 'picture':\n",
        "            content_type = \"AI-Generated Image Description\"\n",
        "            color = 'red'\n",
        "        elif item_type == 'table':\n",
        "            content_type = \"Table (Markdown)\"\n",
        "            color = 'green'\n",
        "        else:\n",
        "            content_type = \"Text Chunk\"\n",
        "            color = 'blue'\n",
        "\n",
        "        print(f\"Content type: {content_type}\")\n",
        "        print(f\"Text preview: {chunk.page_content[:200]}...\")\n",
        "        print(f\"Source: {chunk.metadata.get('source', 'Unknown').split('/')[-1]}\")\n",
        "\n",
        "        # Show visual grounding if available\n",
        "        if \"dl_meta\" in chunk.metadata or \"prov_data\" in chunk.metadata:\n",
        "            visualize_chunk_grounding(\n",
        "                chunk,\n",
        "                doc_store,\n",
        "                highlight_color=color\n",
        "            )\n",
        "        else:\n",
        "            print(\"  (No visual grounding available for this chunk)\")\n",
        "\n",
        "    # Step 3: Create RAG pipeline for response generation\n",
        "    print(\"\\nGenerating response with LLM...\")\n",
        "\n",
        "    # Create Granite prompt template\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        conversation=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"{input}\",\n",
        "        }],\n",
        "        documents=[{\n",
        "            \"doc_id\": \"0\",\n",
        "            \"text\": \"{context}\",\n",
        "        }],\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        template=escape_f_string(prompt, \"input\", \"context\")\n",
        "    )\n",
        "\n",
        "    # Document prompt template\n",
        "    document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
        "<|end_of_text|>\n",
        "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
        "{page_content}\"\"\")\n",
        "\n",
        "    # Create chains\n",
        "    combine_docs_chain = create_stuff_documents_chain(\n",
        "        llm=model,\n",
        "        prompt=prompt_template,\n",
        "        document_prompt=document_prompt_template,\n",
        "        document_separator=\"\",\n",
        "    )\n",
        "\n",
        "    rag_chain = create_retrieval_chain(\n",
        "        retriever=retriever,\n",
        "        combine_docs_chain=combine_docs_chain,\n",
        "    )\n",
        "\n",
        "    # Generate response\n",
        "    outputs = rag_chain.invoke({\"input\": question})\n",
        "\n",
        "    print(\"\\nGenerated Answer:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(outputs['answer'])\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return outputs, relevant_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQitCyY0RVPn"
      },
      "source": [
        "## Demonstrate the Complete System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d__u-q6RVnO"
      },
      "source": [
        "### Main Query with Visual Grounding\n",
        "\n",
        "Let's run our main query and see the complete system in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud0Oy_K4Ra7u"
      },
      "outputs": [],
      "source": [
        "main_query = \"How much was spent compared to the amount of food distributed?\"\n",
        "outputs, chunks = rag_with_visual_grounding(\n",
        "    main_query,\n",
        "    vector_db,\n",
        "    doc_store,\n",
        "    model,\n",
        "    tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABGUdQ5RaX4"
      },
      "source": [
        "# Summary and Next Steps\n",
        "\n",
        "### What You've Accomplished\n",
        "\n",
        "Congratulations! You've successfully built an advanced multimodal RAG system with visual grounding. Here's what you've learned:\n",
        "\n",
        "### Technical Skills Acquired\n",
        "\n",
        "1. **Visual Grounding Implementation**\n",
        "   - Configured Docling for visual reference preservation\n",
        "   - Maintained coordinate metadata through processing pipeline\n",
        "   - Created visual attribution for all content types\n",
        "\n",
        "2. **Multimodal Document Processing**\n",
        "   - Handled text, tables, and images seamlessly\n",
        "   - Used AI vision models for image understanding\n",
        "   - Preserved structure while enabling search\n",
        "\n",
        "3. **Transparent RAG Architecture**\n",
        "   - Built trust through visual verification\n",
        "   - Enabled source attribution for every response\n",
        "   - Created differentiated highlighting for content types\n",
        "\n",
        "4. **Production-Ready Integration**\n",
        "   - Combined multiple AI models effectively\n",
        "   - Implemented robust error handling\n",
        "   - Created scalable document processing pipeline\n",
        "\n",
        "### Key Insights from the Workshop Journey\n",
        "\n",
        "Looking back at all three labs:\n",
        "- **Lab 1**: Document structure preservation enables everything else\n",
        "- **Lab 2**: Intelligent chunking optimizes retrieval quality\n",
        "- **Lab 3**: Visual grounding transforms RAG into transparent AI\n",
        "\n",
        "Each lab built on the previous, culminating in this advanced system.\n",
        "\n",
        "### Why Visual Grounding Changes Everything\n",
        "\n",
        "Traditional RAG systems are \"black boxes\" - users must trust the AI blindly. Your system:\n",
        "- **Shows sources**: Every claim can be visually verified\n",
        "- **Builds trust**: Users see exactly where information comes from\n",
        "- **Enables audit**: Perfect for regulated industries\n",
        "- **Reduces hallucination**: Visual verification catches errors\n",
        "\n",
        "### The Power of Multimodal Understanding\n",
        "\n",
        "By processing text, tables, and images, your system:\n",
        "- Captures complete document information\n",
        "- Enables richer queries and responses\n",
        "- Handles real-world document complexity\n",
        "- Provides comprehensive answers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3zahmhOg0Jd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDxA2ivg0vn"
      },
      "source": [
        "## Next Steps: Where to Go from Here\n",
        "\n",
        "### Immediate Actions\n",
        "\n",
        "1. **Experiment with Your Documents**\n",
        "   - Try documents with complex layouts\n",
        "   - Test with technical diagrams and charts\n",
        "   - Process multi-page reports with mixed content\n",
        "\n",
        "2. **Customize for Your Domain**\n",
        "   ```python\n",
        "   # Example: Domain-specific image prompts\n",
        "   medical_prompt = \"Describe this medical image, noting any abnormalities or key features\"\n",
        "   financial_prompt = \"Analyze this financial chart, identifying trends and key data points\"\n",
        "   ```\n",
        "\n",
        "3. **Optimize Performance**\n",
        "   - Batch process documents\n",
        "   - Implement caching for visual grounding\n",
        "   - Use GPU acceleration for vision models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVPQsnmFg-HB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3UNKhQhI7Z"
      },
      "source": [
        "## Resources for Continued Learning\n",
        "\n",
        "### Official Documentation\n",
        "- **[Docling Documentation](https://github.com/docling-project/docling)**: Latest features and updates\n",
        "- **[IBM Granite Models](https://www.ibm.com/granite/)**: Model cards and capabilities\n",
        "- **[LangChain Docs](https://python.langchain.com/)**: RAG patterns and best practices\n",
        "- **[Milvus Documentation](https://milvus.io/docs)**: Vector database optimization\n",
        "\n",
        "### Community Resources\n",
        "- Join the Docling community on GitHub\n",
        "- Share your implementations\n",
        "- Contribute improvements back to the project\n",
        "\n",
        "### Related Topics to Explore\n",
        "- Document Layout Analysis\n",
        "- Multimodal Embeddings\n",
        "- Visual Question Answering\n",
        "- Explainable AI Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytIk0-fghmMy"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wADjxUDfhm5L"
      },
      "source": [
        "## Final Thoughts\n",
        "\n",
        "You've completed an incredible journey from basic document conversion to building a sophisticated, transparent AI system. The combination of Docling's document understanding, Granite's AI capabilities, and visual grounding creates a powerful application.\n",
        "\n",
        "Your multimodal RAG system with visual grounding represents the cutting edge of document AI. Whether you're building for healthcare, legal, financial, or any other domain, you now have the tools to create AI systems that are not just powerful, but trustworthy and transparent."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}